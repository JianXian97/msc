## MSc Computing (Artificial Intelligence and Machine Learning) Individual Project

### Project Title: Efficient Vision Transformers for 3D Medical Image Segmentation

### Project Plan

#### 1. Objectives

The objective of this project is to develop efficient transformers for medical imaging and segmentation.

#### 2. Data

Medical imaging datasets: 1. Abdominal multi-organ benchmark for versatile medical image segmentation (Amos) and 2. Beyond the Cranial Vault (BTCV) Segmentation Challenge dataset. These datasets will be sourced from the internet.

#### 3. Overall Plan

##### 3.1 Implementing segmentation models

A literature review into deep learning segmentation models would be conducted. The two main models are UNETR and TransBTS, and attempts would be made to reproduce the results reported in these papers. Improvements to these architectures could be implemented. Computational costs in these model architectures will be assessed and tackled using efficient transformer architecture.

##### 3.2 Implementing efficient transformers

Another literature review will be conducted into different efficient transformer architectures. These could include MobileViT, Swin transformers and sparsity based transformers. These architectures will be incorporated into one of the segmentation models to address the high computation costs within these models.

##### 3.3 Implementing novel methods

Further review into novel methods in Transformers to push the boundaries of the efficiency-accuracy trade off. These methods would then be introduced into the model.

#### 4. Timeline

The following lists a tentative plan which may be modified anytime during the project duration based on the progress made.

1. Week 1 to Week 2 (3 Apr – 14 Apr)
Conduct literature review on UNETR/TransBTS networks and implement them in python.
2. Week 3 to Week 4 (17 Apr – 28 Apr)
Overseas
3. Week 5 to Week 6 (1 May – 12 May)
Quantify the computational costs in these segmentation networks. Identify the bottlenecks.
4. Week 7 to Week 8 (15 May – 26 May)
Conduct literature review on efficient transformers. Implement them in python. Background and Progress Report Writing.
5. Week 9 to Week 10 (29 May – 9 Jun)
Introduce efficient transformers into the models. Quantify the accuracy and efficiency of the models. Background and Progress Report Writing.
**Submissions: Background and Progress Report (6 Jun)**
6. Week 11 to Week 12 (12 Jun – 23 Jun)
Continuation of the previous week - introduce efficient transformers into the models. Quantify the accuracy and efficiency of the models.
7. Week 13 to Week 14 (26 Jun – 7 Jul)
Look into the literature on the advances in transformers. Implement ways to further enhance efficiency and accuracy.
8. Week 15 to Week 16 (10 Jul – 21 Jul)
Further investigate ways to enhance efficiency and accuracy. Hyper parameter tuning.
9. Week 17 to Week 18 (24 Jul – 4 Aug)
Ablation studies, report writing, analyse experimental results.
10. Week 19 to Week 20 (7 Aug – 18 Aug)
Report writing, Analyse experimental results, Presentation preparation.
11. Week 21 to Week 22 (21 Aug – 1 Sep)
Report writing, Presentation preparation.
12. Week 23 to Week 24 (4 Sep – 15 Sep)
**Submissions: Final Report (11 Sep)
Submissions: Presentation Slides (15 Sep)
Submissions: Oral Presentation, Project Demonstration (12 - 19 Sep)**
